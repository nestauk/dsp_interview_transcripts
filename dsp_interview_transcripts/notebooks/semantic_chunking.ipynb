{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how we could use the langchain SemanticChunker to split up text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from dsp_interview_transcripts import PROJECT_DIR\n",
    "from dsp_interview_transcripts.utils.data_cleaning import clean_data, convert_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load model\n",
    "small_model = SentenceTransformer('paraphrase-MiniLM-L3-v2')\n",
    "\n",
    "# Embed the target sentence\n",
    "target_sentence = \"Are these instructions clear or do you need any further clarification?\"\n",
    "target_embedding = small_model.encode([target_sentence])\n",
    "\n",
    "# Function to process each conversation\n",
    "def remove_preamble(df, target_embedding=target_embedding, model=small_model):\n",
    "    \"\"\"Get rid of everything up until the bot asks if the instructions are clear\n",
    "    \"\"\"\n",
    "    # Filter BOT messages\n",
    "    bot_messages = df[df['role'] == 'BOT']\n",
    "    \n",
    "    # Embed BOT messages\n",
    "    bot_embeddings = model.encode(bot_messages['text_clean'].tolist())\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(target_embedding, bot_embeddings).flatten()\n",
    "    \n",
    "    # Find the index of the most similar BOT message\n",
    "    most_similar_idx = similarities.argmax()\n",
    "    \n",
    "    # Get the timestamp of that message\n",
    "    cutoff_timestamp = bot_messages.iloc[most_similar_idx]['timestamp_clean']\n",
    "    \n",
    "    # Filter out messages prior to the cutoff timestamp\n",
    "    return df[df['timestamp_clean'] > cutoff_timestamp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the raw data\n",
    "data = pd.read_csv(PROJECT_DIR / 'data/qual_af_transcripts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the text a little and move audio transcriptions to the text column\n",
    "interviews_df = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the conversations are sorted by time, so that the replies go in the right order\n",
    "interviews_df['timestamp_clean'] = interviews_df['timestamp'].apply(convert_timestamp)\n",
    "interviews_df = interviews_df.groupby('conversation', group_keys=False).apply(lambda x: x.sort_values('timestamp_clean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviews_cleaned_df = interviews_df.groupby('conversation').apply(remove_preamble).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(interviews_df) - len(interviews_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(interviews_cleaned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interviews_cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn every conversation into one big block of text (mimics the format of other interview/focus group transcripts we might see)\n",
    "df_grouped = interviews_df.groupby('conversation')['text_clean'].apply(lambda x: '. '.join(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "buffer_sizes = [1, 2, 3]\n",
    "\n",
    "all_results = {}\n",
    "actual_chunks = {}\n",
    "\n",
    "for buffer_size in buffer_sizes:\n",
    "    chunker = SemanticChunker(HuggingFaceEmbeddings(model_name=model_name), \n",
    "                              breakpoint_threshold_type=\"percentile\", \n",
    "                              buffer_size=buffer_size)\n",
    "    results = {}\n",
    "\n",
    "    for idx, row in df_grouped.iterrows():\n",
    "        text = row['text_clean']\n",
    "        conv_id = row['conversation']\n",
    "        # Turn it into a langchain document\n",
    "        doc = Document(page_content=text)\n",
    "        chunked_docs = chunker.split_documents([doc])\n",
    "        results[conv_id] = [x.model_dump() for x in chunked_docs]\n",
    "    \n",
    "    \n",
    "    all_results[buffer_size] = [len(chunk_list) for chunk_list in results.values()]\n",
    "    actual_chunks[buffer_size] = results\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(buffer_sizes), figsize=(15, 5), sharey=True)\n",
    "\n",
    "for ax, buffer_size in zip(axes, buffer_sizes):\n",
    "    lengths = all_results[buffer_size]\n",
    "    ax.hist(lengths, bins=20, alpha=0.7)\n",
    "    ax.set_title(f'Buffer Size {buffer_size}')\n",
    "    ax.set_xlabel('Number of Chunks')\n",
    "    ax.set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Distribution of List Lengths for Different Buffer Sizes', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [the documentation](https://python.langchain.com/docs/how_to/semantic-chunker/) for info on different breakpoints. Percentile is the default.\n",
    "\n",
    "See also [this notebook](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb).\n",
    "\n",
    "Note that [the documentation](https://api.python.langchain.com/en/latest/text_splitter/langchain_experimental.text_splitter.SemanticChunker.html) suggests you can manipulate:\n",
    "* the exact numerical value of the breakpoint threshold\n",
    "* the regex for sentence delimiters\n",
    "* the number of chunks if you have a sense of what this would be for your document\n",
    "\n",
    "`buffer_size` = the number of sentences either side to include. So if `buffer_size` is 1, you will get 3 sentences in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
